{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de27da72",
   "metadata": {},
   "source": [
    "This notebook is set to describe how to run the Shallow Gibbs Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52657d4",
   "metadata": {},
   "source": [
    "Import our dataset: slump dataset (here as our e.g.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "##################################################################################### COPULE DATA ###################################################################################\n",
    "\n",
    "from scipy.io import arff\n",
    "data = arff.loadarff('slump.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "covariables = df.iloc[:,0:7].values\n",
    "response = df.iloc[:,7:10].values\n",
    "positions = np.arange(103)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "covariables_train, covariables_test, response_train, response_test,positions_train,positions_test = train_test_split(covariables, response,positions, test_size=0.33, random_state=42)\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "#Pour le training, build training data\n",
    "xtrain,ytrain = covariables_train, response_train\n",
    "#corrections\n",
    "\n",
    "np.where(np.isnan(ytrain), ma.array(ytrain, mask=np.isnan(ytrain)).mean(axis=0), ytrain)\n",
    "\n",
    "\n",
    "#Pour tester build test data\n",
    "xtest,ytest = covariables_test, response_test \n",
    "\n",
    "#corrections\n",
    "\n",
    "np.where(np.isnan(ytest), ma.array(ytest, mask=np.isnan(ytest)).mean(axis=0), ytest) \n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157200ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "#an integer for the number of neurons on the hidden layer\n",
    "my_l_1 = NHLayer = 2\n",
    "\n",
    "\n",
    "#n integer for the number of epochs training times\n",
    "epoch_times =  args.epoch_times\n",
    "\n",
    "#a float for the learning rate of parameters\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "#an integer for the proportion of batch training data\n",
    "batch_psize = args.batch_psize\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "#an integer for the Number of times we simulate \\lambda_w to estimate the Expected-loglikelihood Score\n",
    "Number_EpLogLike = args.Number_EpLogLike \n",
    "\n",
    "\n",
    "#an integer for the Number of times We simulate  W and b to be used for sampling the predicted test data from the model\n",
    "Simulate_W_b_Pred = 5\n",
    "\n",
    "#an integer for the Number of times we simulate Ytest given [each] W and b\n",
    "Pred_Simulate_Ytest = 5\n",
    "\n",
    "#an integer for the Number of times We simulate  W and b to be used to estimate the probability of acceptation each partition\n",
    "Simulate_Proba_Partition = 5 \n",
    "\n",
    "######################################################################################################################################\n",
    "#covtraining = args.covtraining\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "#an integer for the number of Double Backpropagation Correction\n",
    "DBS = 20\n",
    "\n",
    "#a float for the DBS learning rate of parameters\n",
    "dbs_epsilon = 10e-3\n",
    "\n",
    "#training and testing data\n",
    "\n",
    "Train_PottsData = xtrain\n",
    "Test_PottsData = xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3c5e6",
   "metadata": {},
   "source": [
    "Run an initial Partition first using the pottscompleteshrinkage package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc792198",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_PottsData_demo = xtrain\n",
    "#Import the Potts Complete Shrinkage module\n",
    "import pottsshrinkage.completeshrinkage as PCS\n",
    "#Choose the number of colors\n",
    "q = 20\n",
    "#Compute Initial Potts Clusters as a first Random Partition (with Potts Model)\n",
    "InitialPottsClusters = PCS.InitialPottsConfiguration(Train_PottsData_demo, q, Kernel=\"Mercer\")\n",
    "#Choose your temperature (T) level\n",
    "T = 1000\n",
    "\n",
    "#Set the bandwidth of the model\n",
    "sigma = 1\n",
    "\n",
    "#Set the Number of Random_Partitions you want to simulate\n",
    "Number_of_Random_Partitions = 1\n",
    "\n",
    "#Set your initial (random) Potts partition as computed above\n",
    "Initial_Partition = InitialPottsClusters\n",
    "\n",
    "#Set the Minimum Size desired for each partition generated\n",
    "MinClusterSize = 5\n",
    "\n",
    "#Run your Potts Complete Shrinkage Model to simulate the Randomly Shrunk Potts Partitions. Partitions_Sets is a dictionary that can be saved \n",
    "#with pickle package.\n",
    "Partitions_Sets,Spin_Configuration_Sets = PCS.Potts_Random_Partition (Train_PottsData_demo, T, sigma, Number_of_Random_Partitions, MinClusterSize, Initial_Partition,  Kernel=\"Mercer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872bbe2",
   "metadata": {},
   "source": [
    "Save the initial Partition from Partitions_Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('Initial_Partitions_constraints_1_Sets.pkl.pkl', 'wb')\n",
    "pickle.dump(Partitions_Sets, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c91f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the model in a finite loop for a maximum number of desired partitions such that we stop the loop when completed.\n",
    "\n",
    "The model has to reject the partition that are generated but not accepted by an acceptation ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d64786",
   "metadata": {},
   "outputs": [],
   "source": [
    "Maximum_number_of_partitions = 10\n",
    "\n",
    "for i in range(Maximum_number_of_partitions):\n",
    "\n",
    "    \n",
    "    ShallowGibbsProcedure(New_partition, xtrain, ytrain, Partitions_Sets,partition_position, xtest,ytest,my_l_1,epoch_times, learning_rate,batch_psize,Number_EpLogLike, Simulate_W_b_Pred, Pred_Simulate_Ytest,Simulate_Proba_Partition, DBS, dbs_epsilon)\n",
    "    \n",
    "    acceptation_ratio = ...[]...\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
